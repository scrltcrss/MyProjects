{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def predict_proba_in_batches(model, data, batch_size=100000, predict_mode=\"base\"):\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    probabilities = np.zeros((num_samples,))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        print(f\"Processing batch: {batch_idx+1}/{num_batches}\")\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "        X_batch = data.iloc[start_idx:end_idx]\n",
    "        if predict_mode == \"base\":\n",
    "            batch_probs = model.predict_proba(X_batch)[:, 1]\n",
    "        elif predict_mode == \"lightautoml\":\n",
    "            batch_probs = model.predict(X_batch).data.squeeze()\n",
    "        probabilities[start_idx:end_idx] = batch_probs\n",
    "        gc.collect()\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "class DataPreprocessor:\n",
    "    @staticmethod\n",
    "    def transform_data_types(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_columns(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df\n",
    "\n",
    "# Класс для агрегации данных\n",
    "# Класс для агрегации данных\n",
    "class DataAggregator:\n",
    "    @staticmethod\n",
    "    def get_expressions(df):\n",
    "        numeric_cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        date_cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        string_cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        other_cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        count_cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max_numeric = [pl.max(col).alias(f\"max_{col}\") for col in numeric_cols]\n",
    "        expr_max_date = [pl.max(col).alias(f\"max_{col}\") for col in date_cols]\n",
    "        expr_max_string = [pl.max(col).alias(f\"max_{col}\") for col in string_cols]\n",
    "        expr_max_other = [pl.max(col).alias(f\"max_{col}\") for col in other_cols]\n",
    "        expr_max_count = [pl.max(col).alias(f\"max_{col}\") for col in count_cols]\n",
    "\n",
    "        return expr_max_numeric + expr_max_date + expr_max_string + expr_max_other + expr_max_count\n",
    "\n",
    "\n",
    "# Функции чтения данных\n",
    "def read_data_file(file_path, depth=None):\n",
    "    df = pl.read_parquet(file_path)\n",
    "    df = df.pipe(DataPreprocessor.transform_data_types)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))\n",
    "    \n",
    "    return df\n",
    "def read_files(file_pattern, depth=None):\n",
    "    chunks = []\n",
    "    for file_path in glob(str(file_pattern)):\n",
    "        df = pl.read_parquet(file_path)\n",
    "        df = df.pipe(DataPreprocessor.transform_data_types)\n",
    "        \n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Основная часть кода\n",
    "ROOT            = Path(\"C:\\\\Users\\\\Daniil Bokhan\\\\Desktop\\\\csv\\\\bankcredit-riskCOMPETITION\")\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "# Чтение и предобработка данных для тренировочного набора\n",
    "train_data_store = {\n",
    "    \"df_base\": read_data_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Чтение и предобработка данных для тестового набора\n",
    "test_data_store = {\n",
    "    \"df_base\": read_data_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(DataPreprocessor.process_dates)\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "\n",
    "# Объединение данных для тренировочного и тестового наборов\n",
    "df_train = feature_eng(**train_data_store)\n",
    "df_test = feature_eng(**test_data_store)\n",
    "\n",
    "# Фильтрация колонок\n",
    "df_train = df_train.pipe(DataPreprocessor.filter_columns)\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "\n",
    "# Очистка памяти\n",
    "del train_data_store\n",
    "del test_data_store\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit,train_test_split\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\",\"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, val_index in gss.split(X, y, groups=weeks):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    weeks_train, weeks_val = weeks.iloc[train_index], weeks.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit,train_test_split\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def gini_stability(base, y_true, weeks, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.groupby(weeks)\\\n",
    "                      .apply(lambda x: 2 * roc_auc_score(y_true.loc[x.index], x[\"predictions\"]) - 1)\\\n",
    "                      .tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a * x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "def objective(trial, X_train, X_val, y_train, y_val, weeks_train, weeks_val):\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"random_state\": 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 2000),\n",
    "        'lambda': trial.suggest_float('lambda', 0.01, 1),\n",
    "        'alpha': trial.suggest_float('alpha', 0.01, 1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 16),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 20),\n",
    "        'device':'gpu',\n",
    "    }\n",
    "\n",
    "    lgbm_classifier = LGBMClassifier(**param)\n",
    "    lgbm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = lgbm_classifier.predict_proba(X_val)[:, 1]\n",
    "    auc_score = roc_auc_score(y_val, predictions)\n",
    "\n",
    "    return auc_score\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  \n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "study.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val, weeks_train, weeks_val), n_trials=500)\n",
    "\n",
    "best_params = study.best_params\n",
    "print('='*50)\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
