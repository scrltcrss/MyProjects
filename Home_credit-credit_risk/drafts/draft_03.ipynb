{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def predict_proba_in_batches(model, data, batch_size=100000, predict_mode=\"base\"):\n",
    "    num_samples = len(data)\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "    probabilities = np.zeros((num_samples,))\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        print(f\"Processing batch: {batch_idx+1}/{num_batches}\")\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
    "        X_batch = data.iloc[start_idx:end_idx]\n",
    "        if predict_mode == \"base\":\n",
    "            batch_probs = model.predict_proba(X_batch)[:, 1]\n",
    "        elif predict_mode == \"lightautoml\":\n",
    "            batch_probs = model.predict(X_batch).data.squeeze()\n",
    "        probabilities[start_idx:end_idx] = batch_probs\n",
    "        gc.collect()\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "class DataPreprocessor:\n",
    "    @staticmethod\n",
    "    def transform_data_types(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_columns(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df\n",
    "\n",
    "# Класс для агрегации данных\n",
    "# Класс для агрегации данных\n",
    "class DataAggregator:\n",
    "    @staticmethod\n",
    "    def get_expressions(df):\n",
    "        numeric_cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        date_cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        string_cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        other_cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        count_cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max_numeric = [pl.max(col).alias(f\"max_{col}\") for col in numeric_cols]\n",
    "        expr_max_date = [pl.max(col).alias(f\"max_{col}\") for col in date_cols]\n",
    "        expr_max_string = [pl.max(col).alias(f\"max_{col}\") for col in string_cols]\n",
    "        expr_max_other = [pl.max(col).alias(f\"max_{col}\") for col in other_cols]\n",
    "        expr_max_count = [pl.max(col).alias(f\"max_{col}\") for col in count_cols]\n",
    "\n",
    "        return expr_max_numeric + expr_max_date + expr_max_string + expr_max_other + expr_max_count\n",
    "\n",
    "\n",
    "# Функции чтения данных\n",
    "def read_data_file(file_path, depth=None):\n",
    "    df = pl.read_parquet(file_path)\n",
    "    df = df.pipe(DataPreprocessor.transform_data_types)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))\n",
    "    \n",
    "    return df\n",
    "def read_files(file_pattern, depth=None):\n",
    "    chunks = []\n",
    "    for file_path in glob(str(file_pattern)):\n",
    "        df = pl.read_parquet(file_path)\n",
    "        df = df.pipe(DataPreprocessor.transform_data_types)\n",
    "        \n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Основная часть кода\n",
    "ROOT            = Path(\"C:\\\\Users\\\\Daniil Bokhan\\\\Desktop\\\\csv\\\\bankcredit-riskCOMPETITION\")\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "# Чтение и предобработка данных для тренировочного набора\n",
    "train_data_store = {\n",
    "    \"df_base\": read_data_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_data_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Чтение и предобработка данных для тестового набора\n",
    "test_data_store = {\n",
    "    \"df_base\": read_data_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_data_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(DataPreprocessor.process_dates)\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "\n",
    "# Объединение данных для тренировочного и тестового наборов\n",
    "df_train = feature_eng(**train_data_store)\n",
    "df_test = feature_eng(**test_data_store)\n",
    "\n",
    "# Фильтрация колонок\n",
    "df_train = df_train.pipe(DataPreprocessor.filter_columns)\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "\n",
    "# Очистка памяти\n",
    "del train_data_store\n",
    "del test_data_store\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit,train_test_split\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\",\"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "for train_index, val_index in gss.split(X, y, groups=weeks):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    weeks_train, weeks_val = weeks.iloc[train_index], weeks.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[cat_cols] = df_train[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7202152\tbest: 0.7202152 (0)\ttotal: 3.61s\tremaining: 4h 30m 42s\n",
      "100:\ttest: 0.8088639\tbest: 0.8092508 (91)\ttotal: 3m 35s\tremaining: 2h 36m 45s\n",
      "200:\ttest: 0.8152960\tbest: 0.8152960 (200)\ttotal: 7m 5s\tremaining: 2h 31m 44s\n",
      "300:\ttest: 0.8226839\tbest: 0.8226974 (298)\ttotal: 10m 39s\tremaining: 2h 28m 35s\n",
      "400:\ttest: 0.8277173\tbest: 0.8277173 (400)\ttotal: 14m 10s\tremaining: 2h 24m 56s\n",
      "500:\ttest: 0.8314860\tbest: 0.8314860 (500)\ttotal: 17m 38s\tremaining: 2h 20m 52s\n",
      "600:\ttest: 0.8340490\tbest: 0.8340490 (600)\ttotal: 21m 10s\tremaining: 2h 17m 24s\n",
      "700:\ttest: 0.8362111\tbest: 0.8362111 (700)\ttotal: 24m 44s\tremaining: 2h 14m 6s\n",
      "800:\ttest: 0.8376132\tbest: 0.8376132 (800)\ttotal: 28m 17s\tremaining: 2h 10m 36s\n",
      "900:\ttest: 0.8387954\tbest: 0.8387954 (900)\ttotal: 31m 48s\tremaining: 2h 7m 2s\n",
      "1000:\ttest: 0.8398947\tbest: 0.8398947 (1000)\ttotal: 35m 18s\tremaining: 2h 3m 24s\n",
      "1100:\ttest: 0.8407289\tbest: 0.8407289 (1100)\ttotal: 38m 47s\tremaining: 1h 59m 45s\n",
      "1200:\ttest: 0.8414548\tbest: 0.8414548 (1200)\ttotal: 46m 25s\tremaining: 2h 7m 32s\n",
      "1300:\ttest: 0.8421333\tbest: 0.8421333 (1300)\ttotal: 54m 21s\tremaining: 2h 13m 39s\n",
      "1400:\ttest: 0.8426873\tbest: 0.8426873 (1400)\ttotal: 1h 3m 45s\tremaining: 2h 21m 3s\n",
      "1500:\ttest: 0.8431890\tbest: 0.8431890 (1500)\ttotal: 1h 14m 44s\tremaining: 2h 29m 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 1556, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.8434644496\n",
      "bestIteration = 1555\n",
      "\n",
      "Shrink model to first 1556 iterations.\n",
      "AUC on validation set: 0.8434644496223225\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"boosting_type\": \"Plain\",\n",
    "    \"random_seed\": 42,\n",
    "    'iterations': 4500,\n",
    "    \"learning_rate\": 0.01505646379545813,\n",
    "    \"min_data_in_leaf\": 32,\n",
    "    \"l2_leaf_reg\": 4.955169551336579e-08,\n",
    "    \"random_strength\": 1.5828504506752107e-07,\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n",
    "CatBoostClassifierm = CatBoostClassifier(**params)\n",
    "CatBoostClassifierm.fit(train_pool, eval_set=(X_val, y_val), verbose=100)\n",
    "\n",
    "predictions = CatBoostClassifierm.predict_proba(X_val)[:, 1]\n",
    "auc_score = roc_auc_score(y_val, predictions)\n",
    "print(\"AUC on validation set:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-16 14:40:13,381] A new study created in memory with name: no-name-d9c99e2a-253f-40d8-ae54-f6d5b7d94ae4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit,train_test_split  # Importing GroupShuffleSplit for group-based splitting\n",
    "import optuna  # Importing Optuna for hyperparameter optimization\n",
    "from lightgbm import LGBMClassifier  # Importing LightGBM classifier\n",
    "from sklearn.metrics import roc_auc_score  # Importing ROC AUC score metric\n",
    "\n",
    "# Defining a custom metric to optimize for gini stability\n",
    "def gini_stability(base, y_true, weeks, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    # Computing Gini in time for each week and aggregating\n",
    "    gini_in_time = base.groupby(weeks)\\\n",
    "                      .apply(lambda x: 2 * roc_auc_score(y_true.loc[x.index], x[\"predictions\"]) - 1)\\\n",
    "                      .tolist()\n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a * x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    \n",
    "    # Computing average Gini, falling rate, and residual standard deviation\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "# Defining the objective function for Optuna optimization\n",
    "def objective(trial, X_train, X_val, y_train, y_val, weeks_train, weeks_val):\n",
    "    # Defining the parameters to be optimized along with their ranges\n",
    "    param = {\n",
    "        \"objective\": \"binary\",  # Objective function for optimization - binary classification\n",
    "        \"metric\": \"auc\",  # Quality metric - Area Under ROC Curve\n",
    "        \"verbosity\": -1,  # Verbosity level for training information (-1 to disable)\n",
    "        \"boosting_type\": \"gbdt\",  # Boosting type - Gradient Boosting Decision Trees\n",
    "        \"random_state\": 42,  # Setting random state for result reproducibility\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),  # Learning rate\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 70),  # Maximum number of leaves in a tree\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 16),  # Maximum tree depth\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),  # Minimum data in leaf\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),  # Subsample ratio for training each tree\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),  # Feature fraction for building each tree\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),  # L1 regularization\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),  # L2 regularization\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1200, 2800),  # Number of boosting trees\n",
    "        \"min_split_gain\": trial.suggest_loguniform(\"min_split_gain\", 1e-4, 0.1),  # Minimum loss reduction for split\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),  # Frequency of subsampling for training each tree\n",
    "        \"cat_smooth\": trial.suggest_int(\"cat_smooth\", 10, 50),  # Smoothing for categorical features\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 500),  # Maximum number of bins for histogram construction\n",
    "        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 1, 10)  # Maximum delta step in weight estimation for gradient descent\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initializing LightGBM classifier with the suggested parameters\n",
    "    lgbm_classifier = LGBMClassifier(**params)\n",
    "    \n",
    "    # Training the classifier\n",
    "    lgbm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions on the validation set\n",
    "    predictions = lgbm_classifier.predict_proba(X_val)[:, 1]\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['predictions'])\n",
    "    \n",
    "    # Combining predictions with validation set and weeks information\n",
    "    index_X_val = X_val.index\n",
    "    X_val_with_weeks = X_val.join(weeks_val.rename(\"WEEK_NUM\"))\n",
    "    base = pd.concat([X_val_with_weeks.reset_index(drop=True), predictions_df], axis=1)\n",
    "    base.index = index_X_val\n",
    "    base[\"WEEK_NUM\"] = weeks_val\n",
    "    \n",
    "    # Computing the gini stability score\n",
    "    score = gini_stability(base, y_val, weeks_val)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Using TPESampler for more efficient sampling\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  \n",
    "\n",
    "# Creating an Optuna study to maximize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "# Optimizing the objective function with 2000 trials\n",
    "study.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val, weeks_train, weeks_val), n_trials=2000)\n",
    "\n",
    "# Retrieving the best parameters found during optimization\n",
    "best_params = study.best_params\n",
    "print('='*50)\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.04643712833927281,\n",
       " 'n_estimators': 753,\n",
       " 'reg_alpha': 0.07054742991649575,\n",
       " 'reg_lambda': 0.09472469242094589,\n",
       " 'max_depth': 9,\n",
       " 'num_leaves': 64,\n",
       " 'min_data_in_leaf': 35,\n",
       " 'feature_fraction': 0.49098130041779536,\n",
       " 'bagging_fraction': 0.7868937212510314,\n",
       " 'bagging_freq': 5,\n",
       " 'min_child_samples': 38,\n",
       " 'min_split_gain': 0.2273261520885937,\n",
       " 'min_child_weight': 0.029529028536948946}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.01325784863435542, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01325784863435542\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.05611626584055533, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.05611626584055533\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lgbm1&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric=&#x27;auc&#x27;,\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective=&#x27;binary&#x27;,\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             (&#x27;lgbm2&#x27;,\n",
       "                              LGBMClassifier(col...\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             (&#x27;lgbm5&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric=&#x27;auc&#x27;,\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective=&#x27;binary&#x27;,\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lgbm1&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric=&#x27;auc&#x27;,\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective=&#x27;binary&#x27;,\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             (&#x27;lgbm2&#x27;,\n",
       "                              LGBMClassifier(col...\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             (&#x27;lgbm5&#x27;,\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric=&#x27;auc&#x27;,\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective=&#x27;binary&#x27;,\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "               lambda_l1=0.01325784863435542, lambda_l2=0.05611626584055533,\n",
       "               learning_rate=0.045260996998376685, max_depth=9, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=50, n_estimators=790, num_leaves=69,\n",
       "               objective=&#x27;binary&#x27;, subsample=0.65734872543659, verbosity=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "               lambda_l1=0.01325784863435542, lambda_l2=0.05611626584055533,\n",
       "               learning_rate=0.045260996998376685, max_depth=9, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=50, n_estimators=790, num_leaves=69,\n",
       "               objective=&#x27;binary&#x27;, subsample=0.65734872543659, verbosity=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm3</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "               lambda_l1=0.01325784863435542, lambda_l2=0.05611626584055533,\n",
       "               learning_rate=0.045260996998376685, max_depth=9, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=50, n_estimators=790, num_leaves=69,\n",
       "               objective=&#x27;binary&#x27;, subsample=0.65734872543659, verbosity=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm4</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "               lambda_l1=0.01325784863435542, lambda_l2=0.05611626584055533,\n",
       "               learning_rate=0.045260996998376685, max_depth=9, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=50, n_estimators=790, num_leaves=69,\n",
       "               objective=&#x27;binary&#x27;, subsample=0.65734872543659, verbosity=-1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm5</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "               lambda_l1=0.01325784863435542, lambda_l2=0.05611626584055533,\n",
       "               learning_rate=0.045260996998376685, max_depth=9, metric=&#x27;auc&#x27;,\n",
       "               min_child_samples=50, n_estimators=790, num_leaves=69,\n",
       "               objective=&#x27;binary&#x27;, subsample=0.65734872543659, verbosity=-1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lgbm1',\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric='auc',\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective='binary',\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             ('lgbm2',\n",
       "                              LGBMClassifier(col...\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1)),\n",
       "                             ('lgbm5',\n",
       "                              LGBMClassifier(colsample_bytree=0.41876124042342466,\n",
       "                                             lambda_l1=0.01325784863435542,\n",
       "                                             lambda_l2=0.05611626584055533,\n",
       "                                             learning_rate=0.045260996998376685,\n",
       "                                             max_depth=9, metric='auc',\n",
       "                                             min_child_samples=50,\n",
       "                                             n_estimators=790, num_leaves=69,\n",
       "                                             objective='binary',\n",
       "                                             subsample=0.65734872543659,\n",
       "                                             verbosity=-1))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "best_params_lgbm1 = {\n",
    "    1\n",
    "    }\n",
    "best_params_lgbm2 = {\n",
    "    2\n",
    "    }\n",
    "best_params_lgbm3 = {\n",
    "    3\n",
    "    }\n",
    "best_params_lgbm4 = {\n",
    "    4\n",
    "    }\n",
    "best_params_lgbm5 = {\n",
    "    5\n",
    "    }\n",
    "\n",
    "\n",
    "lgbm_model_1 = LGBMClassifier(**best_params_lgbm1)\n",
    "lgbm_model_2 = LGBMClassifier(**best_params_lgbm2)\n",
    "lgbm_model_3 = LGBMClassifier(**best_params_lgbm3)\n",
    "lgbm_model_4 = LGBMClassifier(**best_params_lgbm4)\n",
    "lgbm_model_5 = LGBMClassifier(**best_params_lgbm5)\n",
    "\n",
    "# Создание VotingClassifier с использованием моделей LGBM\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm1', lgbm_model_1),\n",
    "        ('lgbm2', lgbm_model_2),\n",
    "        ('lgbm3', lgbm_model_3),\n",
    "        ('lgbm4', lgbm_model_4),\n",
    "        ('lgbm5', lgbm_model_5)\n",
    "    ],\n",
    "    voting='soft'  \n",
    ")\n",
    "voting_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
