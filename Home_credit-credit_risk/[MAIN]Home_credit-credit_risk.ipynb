{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Home_Credit_logo.svg/220px-Home_Credit_logo.svg.png\"  style=\"float: left; margin-right: 20px;\">\n",
    "    <h1 style=\"font-size: 24px; font-weight: bold; color: red;\">Home Credit - Credit Risk Model Stability</h1>\n",
    "    <h2 style=\"font-size: 20px; font-weight: bold;\">Daniil Bokhan Solution</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<style>\n",
    "body {\n",
    "    font-family: Arial, sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "    background-color: #f7f7f7;\n",
    "}\n",
    ".container {\n",
    "    max-width: 800px;\n",
    "    margin: 50px auto;\n",
    "    padding: 20px;\n",
    "    background-color: #fff;\n",
    "    border-radius: 8px;\n",
    "    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "h1, h2 {\n",
    "    color: #333;\n",
    "}\n",
    "p {\n",
    "    color: #666;\n",
    "    line-height: 1.6;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"container\">\n",
    "    <h1>Competition Overview</h1>\n",
    "    <h2>Home Credit - Credit Risk Model Stability</h2>\n",
    "    <p><strong>Goal:</strong> The goal of this competition is to predict which clients are more likely to default on their loans. The evaluation will favor solutions that are stable over time.</p>\n",
    "    <p><strong>Who's Interested:</strong> Participation in the competition may offer consumer finance providers a more reliable and longer-lasting way to assess a potential clientâ€™s default risk.</p>\n",
    "    <p><strong>Description:</strong> In the Home Credit - Credit Risk Model Stability competition, participants are tasked with predicting the probability of clients defaulting on their loans. More stable models, maintaining predictive power over time, will be favored in evaluation. This competition is of interest to financial institutions as it allows for the creation of more reliable loan risk assessment models.</p>\n",
    "    <p><strong>Organizer:</strong> The competition is organized by Home Credit, an international consumer finance provider focusing on responsible lending primarily to people with little or no credit history.</p>\n",
    "    <p><strong>Why It Matters:</strong> Assessing potential clients' default risks enables financial institutions to accept more loan applications, potentially improving the lives of individuals historically denied due to lack of credit history.</p>\n",
    "    <p><strong>Timeline:</strong> The competition started February 5, 2024 and will close May 27, 2024.</p>\n",
    "    <p><strong>Prizes:</strong> There are 9 prize categories, including a special prize for model stability. In total on $105,000</p>\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div>\n",
    "        <h2>Dataset Overview</h2>\n",
    "        <div>\n",
    "            <h3>Overview:</h3>\n",
    "            <p>This dataset comprises numerous tables derived from diverse data sources and varying levels of data aggregation. Note: All files listed are available in both .csv and .parquet formats. In total, the dataset files occupy approximately 26.77 GB of storage space</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Base Tables:</h3>\n",
    "            <p>Base tables hold fundamental information about each observation and case_id. The case_id serves as a unique identifier for each observation and is crucial for linking other tables.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Train Files:</h3>\n",
    "            <ul>\n",
    "                <li>train_base.csv</li>\n",
    "                <li>test_base.csv (Note: The hidden test_base.csv contains approximately 90% of the case_id values from train_base.csv)</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Static_0:</h3>\n",
    "            <p><strong>Properties:</strong> depth=0, internal data source</p>\n",
    "            <p><strong>Description:</strong> These tables contain static features directly associated with specific case_id values.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Static_cb_0:</h3>\n",
    "            <p><strong>Properties:</strong> depth=0, external data source</p>\n",
    "            <p><strong>Description:</strong> These tables contain static features obtained from external data sources.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Applprev_1:</h3>\n",
    "            <p><strong>Properties:</strong> depth=1, internal data source</p>\n",
    "            <p><strong>Description:</strong> These tables include information about previous loan applications made by clients.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Other_1:</h3>\n",
    "            <p><strong>Properties:</strong> depth=1, internal data source</p>\n",
    "            <p><strong>Description:</strong> These tables encompass additional information about clients.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Tax Registries:</h3>\n",
    "            <ul>\n",
    "                <li><strong>Tax_registry_a_1:</strong> Properties: depth=1, external data source</li>\n",
    "                <li><strong>Tax_registry_b_1:</strong> Properties: depth=1, external data source</li>\n",
    "                <li><strong>Tax_registry_c_1:</strong> Properties: depth=1, external data source</li>\n",
    "            </ul>\n",
    "            <p><strong>Description:</strong> These tables contain data from different tax registries.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Credit Bureaus:</h3>\n",
    "            <ul>\n",
    "                <li><strong>Credit_bureau_a_1:</strong> Properties: depth=1, external data source</li>\n",
    "                <li><strong>Credit_bureau_b_1:</strong> Properties: depth=1, external data source</li>\n",
    "            </ul>\n",
    "            <p><strong>Description:</strong> These tables include information from various credit bureaus.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Deposit, Person, Debitcard:</h3>\n",
    "            <ul>\n",
    "                <li><strong>Deposit_1:</strong> Properties: depth=1, internal data source</li>\n",
    "                <li><strong>Person_1:</strong> Properties: depth=1, internal data source</li>\n",
    "                <li><strong>Debitcard_1:</strong> Properties: depth=1, internal data source</li>\n",
    "            </ul>\n",
    "            <p><strong>Description:</strong> These tables hold information about deposits, personal details, and debit cards of clients.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Additional Tables:</h3>\n",
    "            <ul>\n",
    "                <li><strong>Applprev_2:</strong> Properties: depth=2, internal data source</li>\n",
    "                <li><strong>Person_2:</strong> Properties: depth=2, internal data source</li>\n",
    "                <li><strong>Credit_bureau_a_2:</strong> Properties: depth=2, external data source</li>\n",
    "                <li><strong>Credit_bureau_b_2:</strong> Properties: depth=2, external data source</li>\n",
    "            </ul>\n",
    "            <p><strong>Description:</strong> These tables provide deeper insights into previous loan applications, personal details, and additional credit bureau data with a higher level of detail.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h3>Additional Information:</h3>\n",
    "            <ul>\n",
    "                <li>The same naming conventions apply to test files.</li>\n",
    "                <li>Some external data providers might not be available for future evaluations.</li>\n",
    "                <li>Tables are divided based on WEEK_NUM to manage their size.</li>\n",
    "                <li>Depth values indicate the level of data aggregation.</li>\n",
    "                <li>Special columns like case_id, date_decision, WEEK_NUM, etc., provide essential information for analysis.</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Data Schema</h2>\n",
    "            <p align=\"center\">\n",
    "                <a href=\"https://ibb.co/Gvk0RvT\"><img src=\"https://i.ibb.co/4ZM7pZd/schema.png\" alt=\"schema\" border=\"0\"></a> width=\"100%\">\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "        }\n",
    "        h2 {\n",
    "            color: #333;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .metric-section {\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .metric-section h3 {\n",
    "            color: #666;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        .metric-section p {\n",
    "            color: #444;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Evaluation Metric</h2>\n",
    "    <div class=\"metric-section\">\n",
    "        <h3>Gini Score Calculation:</h3>\n",
    "        <p>The Gini score is calculated for predictions corresponding to each WEEK_NUM using the formula: <strong>gini = 2 * AUC - 1</strong>. This measures the predictive power of the model.</p>\n",
    "    </div>\n",
    "    <div class=\"metric-section\">\n",
    "        <h3>Falling Rate Calculation:</h3>\n",
    "        <p>A linear regression is fit through the weekly Gini scores, and a falling rate is calculated as <strong>min(0, a)</strong>, where <em>a</em> is the coefficient of the linear regression. This penalizes models that exhibit a decline in predictive ability over time.</p>\n",
    "    </div>\n",
    "    <div class=\"metric-section\">\n",
    "        <h3>Variability Penalty:</h3>\n",
    "        <p>The variability of predictions is assessed by computing the standard deviation of residuals from the linear regression. A penalty is applied to models with higher variability.</p>\n",
    "    </div>\n",
    "    <div class=\"metric-section\">\n",
    "        <h3>Final Metric Calculation:</h3>\n",
    "        <p>The stability metric is calculated as follows:</p>\n",
    "        <p><strong>stability = mean(gini) + 88.0 * min(0, a) - 0.5 * std(residuals)</strong></p>\n",
    "        <p>This metric combines the average Gini score, falling rate, and penalty for variability to provide an overall measure of stability. Higher stability values indicate more effective models.</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Select Strategy for Problem Solving</h1>\n",
    "    <blockquote>\n",
    "        <p><i>Note: I won't explain all the details during the solution process because I don't consider it appropriate, but I'll briefly describe all the processes using comments.</i></p>\n",
    "    </blockquote>\n",
    "    <p>During the competition, it was found that the best results in this task according to the \"gini stability\" metric are obtained by Blends (CatBoost+LGBM), and all leading positions use them. This, by the way, does not play into the hands of the organizers because it is not entirely clear how secondary validation of models will be conducted in case of victory of participants using Blends, since these models are completely non-interpretable. However, we are only concerned with the final score, so we will use Blends.</p>\n",
    "    <p>Our models are very resource-efficient, and we may encounter an 'out of memory' error for this reason. Therefore, we will not limit ourselves to one notebook; we will reproduce the LGBM model in one notebook, CatBoost in the second, save and load the models, import and blend them in the final third notebook. We will tune the parameters on a stationary PC using Optuna.</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            background-color: #f8f9fa;\n",
    "            color: #333;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 800px;\n",
    "            margin: 20px auto;\n",
    "            padding: 20px;\n",
    "            background-color: #fff;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        h1, h2, h3, h4, h5, h6 {\n",
    "            color: #007bff;\n",
    "        }\n",
    "        h2 {\n",
    "            margin-top: 30px;\n",
    "        }\n",
    "        ol {\n",
    "            margin-top: 10px;\n",
    "        }\n",
    "        li {\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Plan</h1>\n",
    "        <ol>\n",
    "            <li>\n",
    "                <h2>Basics</h2>\n",
    "                <ul>\n",
    "                    <li>Data Loading and Preprocessing</li>\n",
    "                    <li>Reading Data from Parquet Files</li>\n",
    "                    <li>Data Type Conversion and Handling Missing Values</li>\n",
    "                    <li>Feature Engineering Based on Existing Data</li>\n",
    "                    <li>Memory Usage Optimization for Accelerating Computations</li>\n",
    "                    <li>Splitting into Validation Set by 'week_nums'</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <h2>Parameter Search</h2>\n",
    "                <ul>\n",
    "                    <li>Using 'Optuna' to Search for Model Hyperparameters based roc_auc_score for CatBoostClassifier and LGBMClassifier</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <h2>Model Saving and Cross-Validation</h2>\n",
    "                <ul>\n",
    "                    <li>Repeating all the basics for two new notebooks</li>\n",
    "                    <li>Setting Parameters Obtained Using 'Optuna'</li>\n",
    "                    <li>Conducting Cross-Validation and Saving the Model with Access via Link</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <h2>Final Version</h2>\n",
    "                <ul>\n",
    "                    <li>Repeating the basics for the final notebook with replacement on the test dataset</li>\n",
    "                    <li>Creating VotingModel Class</li>\n",
    "                    <li>Saving the Predictions</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <h2>Conclusion</h2>\n",
    "                <ul>\n",
    "                    <li>Sending the Response</li>\n",
    "                    <li>Waiting for Notebook Acceptance</li>\n",
    "                    <li>Comparing Scores</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            background-color: #f8f9fa;\n",
    "            color: #333;\n",
    "        }\n",
    "        .container {\n",
    "            max-width: 800px;\n",
    "            margin: 20px auto;\n",
    "            padding: 20px;\n",
    "            background-color: #fff;\n",
    "            border-radius: 8px;\n",
    "            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        h1, h2, h3, h4, h5, h6 {\n",
    "            color: #007bff;\n",
    "        }\n",
    "        h1 {\n",
    "            font-size: 32px;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        h2 {\n",
    "            font-size: 28px;\n",
    "            margin-top: 30px;\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "        ol {\n",
    "            margin-top: 10px;\n",
    "        }\n",
    "        li {\n",
    "            margin-bottom: 10px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Optuna Notebooks</h1>\n",
    "        <h2>Basics</h2>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os  # Importing the os module to interact with the operating system\n",
    "import gc  # Importing the garbage collection module to manage memory\n",
    "from glob import glob  # Importing the glob module to search for files recursively\n",
    "from pathlib import Path  # Importing the Path class from pathlib module for working with file paths\n",
    "from datetime import datetime  # Importing datetime module for date and time manipulation\n",
    "\n",
    "import numpy as np  # Importing numpy for numerical computing\n",
    "import pandas as pd  # Importing pandas for data manipulation and analysis\n",
    "import polars as pl  # Importing polars for data manipulation and analysis (similar to pandas)\n",
    "\n",
    "import matplotlib.pyplot as plt  # Importing matplotlib for data visualization\n",
    "import seaborn as sns  # Importing seaborn for enhanced data visualization\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold  # Importing StratifiedGroupKFold for cross-validation\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin  # Importing base classes for building custom estimators\n",
    "\n",
    "import lightgbm as lgb  # Importing LightGBM for gradient boosting framework\n",
    "\n",
    "import warnings  # Importing warnings module to handle warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore FutureWarnings\n",
    "\n",
    "# Function to predict probabilities in batches\n",
    "def predict_proba_in_batches(model, data, batch_size=100000, predict_mode=\"base\"):\n",
    "    num_samples = len(data)  # Number of samples in the data\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))  # Number of batches required\n",
    "    probabilities = np.zeros((num_samples,))  # Initializing an array to store probabilities\n",
    "\n",
    "    # Loop through batches\n",
    "    for batch_idx in range(num_batches):\n",
    "        print(f\"Processing batch: {batch_idx+1}/{num_batches}\")  # Printing progress\n",
    "        start_idx = batch_idx * batch_size  # Start index of the batch\n",
    "        end_idx = min((batch_idx + 1) * batch_size, num_samples)  # End index of the batch\n",
    "        X_batch = data.iloc[start_idx:end_idx]  # Getting the batch of data\n",
    "        if predict_mode == \"base\":\n",
    "            batch_probs = model.predict_proba(X_batch)[:, 1]  # Predicting probabilities\n",
    "        elif predict_mode == \"lightautoml\":\n",
    "            batch_probs = model.predict(X_batch).data.squeeze()  # Predicting probabilities (LightAutoML)\n",
    "        probabilities[start_idx:end_idx] = batch_probs  # Storing probabilities\n",
    "        gc.collect()  # Performing garbage collection to free up memory\n",
    "\n",
    "    return probabilities  # Returning the predicted probabilities\n",
    "\n",
    "# Class for preprocessing data\n",
    "class DataPreprocessor:\n",
    "    # Method to transform data types\n",
    "    @staticmethod\n",
    "    def transform_data_types(df):\n",
    "        for col in df.columns:\n",
    "            # Casting columns to Int32 data type\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            # Casting date columns to Date data type\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            # Casting columns ending with \"P\" or \"A\" to Float64 data type\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            # Casting columns ending with \"M\" to String data type\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            # Casting columns ending with \"D\" to Date data type\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    # Method to process date columns\n",
    "    @staticmethod\n",
    "    def process_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Computing date differences\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())  # Converting dates to total days\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))  # Casting to Float32 data type\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")  # Dropping unnecessary columns\n",
    "\n",
    "        return df\n",
    "    \n",
    "    # Method to filter columns based on null values and frequency\n",
    "    @staticmethod\n",
    "    def filter_columns(df):\n",
    "        for col in df.columns:\n",
    "            # Dropping columns with high null percentage\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            # Dropping categorical columns with very low or high cardinality\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df\n",
    "\n",
    "# Class for aggregating data\n",
    "class DataAggregator:\n",
    "    # Method to get aggregation expressions\n",
    "    @staticmethod\n",
    "    def get_expressions(df):\n",
    "        numeric_cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        date_cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        string_cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        other_cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        count_cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max_numeric = [pl.max(col).alias(f\"max_{col}\") for col in numeric_cols]\n",
    "        expr_max_date = [pl.max(col).alias(f\"max_{col}\") for col in date_cols]\n",
    "        expr_max_string = [pl.max(col).alias(f\"max_{col}\") for col in string_cols]\n",
    "        expr_max_other = [pl.max(col).alias(f\"max_{col}\") for col in other_cols]\n",
    "        expr_max_count = [pl.max(col).alias(f\"max_{col}\") for col in count_cols]\n",
    "\n",
    "        return expr_max_numeric + expr_max_date + expr_max_string + expr_max_other + expr_max_count\n",
    "\n",
    "# Function to read a single data file\n",
    "def read_data_file(file_path, depth=None):\n",
    "    df = pl.read_parquet(file_path)  # Reading data from parquet file\n",
    "    df = df.pipe(DataPreprocessor.transform_data_types)  # Preprocessing data types\n",
    "    \n",
    "    if depth in [1, 2]:  # Checking depth for aggregation\n",
    "        df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))  # Aggregating data\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to read multiple data files\n",
    "def read_files(file_pattern, depth=None):\n",
    "    chunks = []\n",
    "    for file_path in glob(str(file_pattern)):  # Iterating through files matching the pattern\n",
    "        df = pl.read_parquet(file_path)  # Reading data from parquet file\n",
    "        df = df.pipe(DataPreprocessor.transform_data_types)  # Preprocessing data types\n",
    "        \n",
    "        if depth in [1, 2]:  # Checking depth for aggregation\n",
    "            df = df.group_by(\"case_id\").agg(DataAggregator.get_expressions(df))  # Aggregating data\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")  # Concatenating dataframes\n",
    "    df = df.unique(subset=[\"case_id\"])  # Removing duplicate case_ids\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main part of the code\n",
    "ROOT = Path(\"C:\\\\Users\\\\Daniil Bokhan\\\\Desktop\\\\csv\\\\bankcredit-riskCOMPETITION\")  # Root directory\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"  # Directory containing training data\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\"  # Directory containing testing data\n",
    "\n",
    "# Reading and preprocessing training data\n",
    "train_data_store = {\n",
    "    \"df_base\": read_data_file(TRAIN_DIR / \"train_base.parquet\"),  # Base training data\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),  # Static training data depth 0\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),  # Static training data depth 0\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),  # Application previous data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),  # Credit bureau data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),  # Credit bureau data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_other_1.parquet\", 1),  # Other data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_person_1.parquet\", 1),  # Person data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),  # Deposit data depth 1\n",
    "        read_data_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),  # Debit card data depth 1\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),  # Credit bureau data depth 2\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),  # Credit bureau data depth 2\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Reading and preprocessing testing data\n",
    "test_data_store = {\n",
    "    \"df_base\": read_data_file(TEST_DIR / \"test_base.parquet\"),  # Base testing data\n",
    "    \"depth_0\": [\n",
    "        read_data_file(TEST_DIR / \"test_static_cb_0.parquet\"),  # Static testing data depth 0\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),  # Static testing data depth 0\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),  # Application previous data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),  # Tax registry data depth 1\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),  # Credit bureau data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),  # Credit bureau data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_other_1.parquet\", 1),  # Other data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_person_1.parquet\", 1),  # Person data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_deposit_1.parquet\", 1),  # Deposit data depth 1\n",
    "        read_data_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),  # Debit card data depth 1\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_data_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),  # Credit bureau data depth 2\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),  # Credit bureau data depth 2\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Feature engineering\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),  # Extracting month from date_decision\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),  # Extracting weekday from date_decision\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):  # Looping through different depths of data\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")  # Joining dataframes\n",
    "        \n",
    "    df_base = df_base.pipe(DataPreprocessor.process_dates)  # Processing date columns\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "# Merging and processing training and testing data\n",
    "df_train = feature_eng(**train_data_store)  # Feature engineering for training data\n",
    "df_test = feature_eng(**test_data_store)  # Feature engineering for testing data\n",
    "\n",
    "# Filtering columns\n",
    "df_train = df_train.pipe(DataPreprocessor.filter_columns)  # Filtering columns in training data\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])  # Selecting columns in testing data\n",
    "\n",
    "# Converting dataframes to pandas and handling categorical columns\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()  # Converting to pandas dataframe\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)  # Getting categorical columns\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")  # Converting categorical columns to category type\n",
    "    \n",
    "    return df_data, cat_cols\n",
    "\n",
    "df_train, cat_cols = to_pandas(df_train)  # Converting training data to pandas and handling categorical columns\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)  # Converting testing data to pandas and handling categorical columns\n",
    "\n",
    "# Clearing memory\n",
    "del train_data_store\n",
    "del test_data_store\n",
    "gc.collect()  # Performing garbage collection to free up memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Splitting</h1>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split  # Importing GroupShuffleSplit and train_test_split\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\",\"WEEK_NUM\"])  # Extracting features for training\n",
    "y = df_train[\"target\"]  # Extracting target variable for training\n",
    "weeks = df_train[\"WEEK_NUM\"]  # Extracting week numbers for grouping\n",
    "\n",
    "# Using GroupShuffleSplit for splitting data into train and validation sets while preserving groups\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)  # Initializing GroupShuffleSplit\n",
    "\n",
    "#A large percentage of the validation set because during parameter search, there was insufficient validation with only 20%\n",
    "\n",
    "# Iterating through train and validation splits\n",
    "for train_index, val_index in gss.split(X, y, groups=weeks):\n",
    "    # Splitting features and target for training and validation sets\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]  # Features for training and validation\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]  # Target variable for training and validation\n",
    "    weeks_train, weeks_val = weeks.iloc[train_index], weeks.iloc[val_index]  # Week numbers for training and validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Optuna</h1>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import optuna  # Importing Optuna for hyperparameter optimization\n",
    "from lightgbm import LGBMClassifier  # Importing LightGBM classifier\n",
    "from sklearn.metrics import roc_auc_score  # Importing ROC AUC score metric\n",
    "\n",
    "\n",
    "def objective(trial, X_train, X_val, y_train, y_val):\n",
    "    param = {\n",
    "        \"objective\": \"binary\",  # Objective function for optimization - binary classification\n",
    "        \"metric\": \"auc\",  # Quality metric - Area Under ROC Curve\n",
    "        \"verbosity\": -1,  # Verbosity level for training information (-1 to disable)\n",
    "        \"boosting_type\": \"gbdt\",  # Boosting type - Gradient Boosting Decision Trees\n",
    "        \"random_state\": 42,  # Setting random state for result reproducibility\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),  # Learning rate\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 70),  # Maximum number of leaves in a tree\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 16),  # Maximum tree depth\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),  # Minimum data in leaf\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),  # Subsample ratio for training each tree\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),  # Feature fraction for building each tree\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),  # L1 regularization\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),  # L2 regularization\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1200, 2800),  # Number of boosting trees\n",
    "        \"min_split_gain\": trial.suggest_loguniform(\"min_split_gain\", 1e-4, 0.1),  # Minimum loss reduction for split\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),  # Frequency of subsampling for training each tree\n",
    "        \"cat_smooth\": trial.suggest_int(\"cat_smooth\", 10, 50),  # Smoothing for categorical features\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 500),  # Maximum number of bins for histogram construction\n",
    "        \"max_delta_step\": trial.suggest_int(\"max_delta_step\", 1, 10)  # Maximum delta step in weight estimation for gradient descent\n",
    "    }\n",
    "\n",
    "    # Initializing LightGBM classifier with the suggested parameters\n",
    "    lgbm_classifier = LGBMClassifier(**param)\n",
    "    \n",
    "    # Training the classifier\n",
    "    lgbm_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions on the validation set\n",
    "    predictions = lgbm_classifier.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculating the AUC score\n",
    "    auc_score = roc_auc_score(y_val, predictions)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "# Using TPESampler for more efficient sampling\n",
    "sampler = optuna.samplers.TPESampler(seed=42)  \n",
    "\n",
    "# Creating an Optuna study to maximize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "# Optimizing the objective function with 2000 trials\n",
    "study.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val, weeks_train, weeks_val), n_trials=2000)\n",
    "\n",
    "# Retrieving the best parameters found during optimization\n",
    "best_params = study.best_params\n",
    "print('='*50)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <p></strong>Copy the output and switch to the Kaggle notebook for the LGBM model.</strong></p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Kaggle notebook LGBM</h1>\n",
    "        <h2>Basics</h2>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys  # System-specific parameters and functions\n",
    "from pathlib import Path  # Object-oriented filesystem paths\n",
    "import subprocess  # Subprocess management\n",
    "import os  # Miscellaneous operating system interfaces\n",
    "import gc  # Garbage collection\n",
    "from glob import glob  # Unix-style pathname pattern expansion\n",
    "from tqdm import tqdm_notebook  # Instantly make your loops show a smart progress meter\n",
    "import numpy as np  # Fundamental package for scientific computing with Python\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import polars as pl  # Fast DataFrame library implemented in Rust and based on Apache Arrow\n",
    "from datetime import datetime  # Basic date and time types\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import matplotlib.pyplot as plt  # Plotting library for the Python programming language\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold  # Model selection\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Base classes for all estimators\n",
    "from sklearn.metrics import roc_auc_score  # Receiver Operating Characteristic (ROC) metric\n",
    "import lightgbm as lgb  # Gradient boosting framework\n",
    "\n",
    "from imblearn.over_sampling import SMOTE  # SMOTE algorithm\n",
    "from sklearn.preprocessing import OrdinalEncoder  # Encode categorical features as an integer array\n",
    "from sklearn.impute import KNNImputer  # Imputation for completing missing values\n",
    "\n",
    "import warnings  # Warning control system\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'  # Root directory\n",
    "\n",
    "# Pipeline class for data processing\n",
    "class Pipeline:\n",
    "    # Function to set table data types\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    # Function to handle dates\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Calculate time difference\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())  # Transform dates to total days\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")  # Drop unnecessary columns\n",
    "        return df\n",
    "\n",
    "    # Function to filter columns\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()  # Calculate proportion of missing values\n",
    "                if isnull > 0.7:  # If proportion exceeds 70%\n",
    "                    df = df.drop(col)  # Drop column\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()  # Count unique values\n",
    "                if (freq == 1) | (freq > 200):  # If unique values are either 1 or greater than 200\n",
    "                    df = df.drop(col)  # Drop column\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Aggregator class for feature aggregation\n",
    "class Aggregator:\n",
    "    # Numeric expression function\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Max value\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Last value\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Mean value\n",
    "        return expr_max + expr_last + expr_mean\n",
    "\n",
    "    # Date expression function\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Max date\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Last date\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Mean date\n",
    "        return expr_max + expr_last + expr_mean\n",
    "\n",
    "    # String expression function\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Max string\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Last string\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Other expression function\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Max value\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Last value\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Count expression function\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Max value\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Last value\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    # Get expressions function\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs\n",
    "\n",
    "# Function to read a single file\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)  # Read Parquet file\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)  # Set data types\n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))  # Aggregate data\n",
    "    return df\n",
    "\n",
    "# Function to read multiple files based on regex pattern\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)  # Read Parquet file\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)  # Set data types\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))  # Aggregate data\n",
    "        chunks.append(df)\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")  # Concatenate data\n",
    "    df = df.unique(subset=[\"case_id\"])  # Remove duplicates\n",
    "    return df\n",
    "\n",
    "# Function for feature engineering\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),  # Extract month from date\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),  # Extract weekday from date\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")  # Join dataframes\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)  # Handle dates\n",
    "    return df_base\n",
    "\n",
    "# Function to convert Polars DataFrame to Pandas DataFrame\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()  # Convert to Pandas DataFrame\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")  # Convert categorical columns to category type\n",
    "    return df_data, cat_cols\n",
    "\n",
    "# Function to reduce memory usage of DataFrame\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Initial memory usage\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # Final memory usage\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "ROOT            = Path(ROOT)  # Convert to Path object\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"  # Train directory\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"  # Test directory\n",
    "\n",
    "data_store = {  # Data storage dictionary\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),  # Base DataFrame\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),  # Static DataFrame - depth 0\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),  # Static DataFrame - depth 0\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),  # Application previous DataFrame - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),  # Tax registry DataFrame A - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),  # Tax registry DataFrame B - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),  # Tax registry DataFrame C - depth 1\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),  # Credit bureau DataFrame A - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),  # Credit bureau DataFrame B - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),  # Other DataFrame - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),  # Person DataFrame - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),  # Deposit DataFrame - depth 1\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),  # Debit card DataFrame - depth 1\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),  # Credit bureau DataFrame B - depth 2\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),  # Credit bureau DataFrame A - depth 2\n",
    "        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),  # Application previous DataFrame - depth 2\n",
    "        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)  # Person DataFrame - depth 2\n",
    "    ]\n",
    "}\n",
    "df_train = feature_eng(**data_store)  # Perform feature engineering\n",
    "print(\"train data shape:\\t\", df_train.shape)  # Display shape of the train data\n",
    "del data_store  # Delete data_store dictionary to free memory\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)  # Filter columns\n",
    "gc.collect()  # Garbage collection\n",
    "\n",
    "df_train, cat_cols = to_pandas(df_train)  # Convert to Pandas DataFrame\n",
    "df_train = reduce_mem_usage(df_train)  # Reduce memory usage\n",
    "print(\"train data shape:\\t\", df_train.shape)  # Display shape of the train data\n",
    "\n",
    "nums=df_train.select_dtypes(exclude='category').columns  # Numeric columns\n",
    "from itertools import combinations, permutations  # Combinatoric iterators\n",
    "\n",
    "nans_df = df_train[nums].isna()  # Missing values DataFrame\n",
    "nans_groups={}  # Dictionary to store columns grouped by number of missing values\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()  # Count missing values\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)  # Add column to group\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]  # Create new group\n",
    "del nans_df; x=gc.collect()  # Delete DataFrame to free memory\n",
    "\n",
    "# Function to reduce group of columns\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        use.append(vx)  # Append column with highest number of unique values\n",
    "    print('Use these',use)\n",
    "    return use\n",
    "\n",
    "# Function to group columns by correlation\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    # Calculate column correlations\n",
    "    correlation_matrix = matrix.corr()\n",
    "\n",
    "    # Group columns\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "    else:\n",
    "        uses=uses+v\n",
    "print('####### NAN count =',k)\n",
    "print(uses)  # Columns to use\n",
    "print(len(uses))  # Number of columns to use\n",
    "\n",
    "# Add categorical columns to the list of columns to use\n",
    "uses=uses+list(df_train.select_dtypes(include='category').columns)\n",
    "print(len(uses))  # Number of columns to use after adding categorical columns\n",
    "df_train=df_train[uses]  # Filter DataFrame to include only selected columns\n",
    "y = df_train[\"target\"]  # Target variable\n",
    "weeks = df_train[\"WEEK_NUM\"]  # Weeks variable\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])  # Drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h2>Model</h2>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_split = 5  # Number of splits for stratified group k-fold cross-validation\n",
    "cv = StratifiedGroupKFold(n_splits=n_split, shuffle=False)  # Define the cross-validation strategy\n",
    "\n",
    "params = {  # LightGBM model hyperparameters\n",
    "    \"boosting_type\": \"gbdt\",  # Boosting type - Gradient Boosting Decision Trees\n",
    "    \"objective\": \"binary\",  # Objective function for optimization - binary classification\n",
    "    \"metric\": \"auc\",  # Quality metric - Area Under ROC Curve\n",
    "    'learning_rate': 0.01505646379545813,  # Learning rate\n",
    "    'num_leaves': 58,  # Maximum number of leaves in a tree\n",
    "    'max_depth': 9,  # Maximum tree depth\n",
    "    'min_data_in_leaf': 32,  # Minimum data in leaf\n",
    "    'subsample': 0.5228576288563273,  # Subsample ratio for training each tree\n",
    "    'colsample_bytree': 0.9660562727545648,  # Feature fraction for building each tree\n",
    "    'reg_alpha': 4.955169551336579e-08,  # L1 regularization\n",
    "    'reg_lambda': 1.5828504506752107e-07,  # L2 regularization\n",
    "    'n_estimators': 2762,  # Number of boosting trees\n",
    "    'min_split_gain': 0.0014131574686610997,  # Minimum loss reduction for split\n",
    "    'subsample_freq': 1,  # Frequency of subsampling for training each tree\n",
    "    'cat_smooth': 41,  # Smoothing for categorical features\n",
    "    'max_bin': 235,  # Maximum number of bins for histogram construction\n",
    "    'max_delta_step': 1,  # Maximum delta step in weight estimation for gradient descent\n",
    "}\n",
    "\n",
    "\n",
    "fitted_models = []  # List to store fitted models\n",
    "cv_scores = []  # List to store cross-validation scores\n",
    "\n",
    "# Perform cross-validation\n",
    "for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "    \n",
    "    # Initialize and train LightGBM model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set = [(X_valid, y_valid)],\n",
    "        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)]\n",
    "    )\n",
    "    \n",
    "    # Store the fitted model\n",
    "    fitted_models.append(model)\n",
    "    \n",
    "    # Predict probabilities for validation set and calculate AUC score\n",
    "    y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    \n",
    "    # Store the cross-validation score\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "# Print cross-validation results\n",
    "print(\"CV AUC scores: \", cv_scores)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h2>Save</h2>\n",
    "    </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl  # Import the pickle module\n",
    "\n",
    "# Iterate over each fitted model\n",
    "for i in range(n_split):\n",
    "    # Open a pickle file in binary write mode\n",
    "    with open(f'/kaggle/working/home-credit-lgb/model_{i}.pkl', 'wb') as fout:\n",
    "        # Dump the fitted model into the pickle file\n",
    "        pkl.dump(fitted_models[i], fout)\n",
    "\n",
    "# Print a message indicating the completion of saving the models\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>After saving the model in a pkl file, we upload it to private access. We repeat all the actions performed on the LGBMClassifier but now for the CatBoostClassifier, starting from parameter tuning and ending with saving and uploading the model. Since the code only differs in the model parameters and the specification of categorical features, I will rewrite only the part related to saving the CatBoostClassifier model.</strong></p>\n",
    "<h1>CatBoostClassifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "params = {\n",
    "    \"eval_metric\": \"AUC\",  \n",
    "     \"depth\": 10,  \n",
    "    \"learning_rate\": 0.03,\n",
    "    \"iterations\": 5363,\n",
    "     \"random_seed\": 3107,  \n",
    "     \"l2_leaf_reg\": 10,  \n",
    "     \"border_count\": 254, \n",
    "    \"verbose\": False,  \n",
    "    \"task_type\": \"GPU\", \n",
    "}\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "fitted_models = []\n",
    "cv_scores = []\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "step = 0\n",
    "for idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#   Because it takes a long time to divide the data set, \n",
    "    step += 1\n",
    "    print(f'current step: {step}')\n",
    "    \n",
    "    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# each time the data set is divided, two models are trained to each other twice, which saves time.\n",
    "    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    train_pool = Pool(X_train, y_train,cat_features=cat_cols)\n",
    "    val_pool = Pool(X_valid, y_valid,cat_features=cat_cols)\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=val_pool, verbose=1000)\n",
    "\n",
    "    \n",
    "    fitted_models.append(model)\n",
    "    y_pred_valid = model.predict_proba(X_valid)[:,1]\n",
    "    auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "    cv_scores.append(auc_score)\n",
    "    \n",
    "print(\"CV AUC scores: \", cv_scores)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "for i in range(n_splits):\n",
    "    with open(f'/kaggle/working/home-credit-cat/model_{i}.pkl', 'wb') as fout:\n",
    "       pkl.dump(fitted_models[i], fout)\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Final Notebook</h1>\n",
    "<p>We're almost at the end. We'll format the test data to the required format and initialize the VotingModel.</p>\n",
    "    <blockquote>\n",
    "        <p><i>Note: I replaced some variable and class names to make the final submit look more unique and avoid typical names. However, the functionality of the code remains unchanged except for blending, of course<i><p>\n",
    "    </blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import joblib  # Importing joblib for saving and loading models\n",
    "from pathlib import Path  # Importing Path for path operations\n",
    "import gc  # Importing gc for garbage collection\n",
    "from glob import glob  # Importing glob for file matching\n",
    "import numpy as np  # Importing numpy for numerical operations\n",
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "import polars as pl  # Importing polars for fast data manipulation\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Importing BaseEstimator and RegressorMixin base classes\n",
    "from sklearn.metrics import roc_auc_score  # Importing roc_auc_score for metric calculation\n",
    "import lightgbm as lgb  # Importing lightgbm for gradient boosting\n",
    "import pickle  # Importing pickle for saving and loading models\n",
    "import warnings  # Importing warnings for warning handling\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'  # Root directory\n",
    "\n",
    "# Custom Pipeline class for data preprocessing\n",
    "class CustomPipeline:\n",
    "    def dtype_table(df):  # Method for setting table data types\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def date_handling(df):  # Method for handling dates\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  # Calculate time differences\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())  # Convert time differences to total days\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")  # Drop unnecessary columns\n",
    "        return df\n",
    "\n",
    "    def column_filter(df):  # Method for filtering columns\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)  # Drop columns with more than 70% missing values\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)  # Drop columns with only one unique value or more than 200 unique values\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Custom Aggregator class for feature aggregation\n",
    "class CustomAggregator:\n",
    "    def numerical_expr(df):  # Method for numerical expressions\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]  # Select numerical columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]  # Calculate variance\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def date_expr(df):  # Method for date expressions\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]  # Select date columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]  # Calculate mean\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]  # Calculate median\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def string_expr(df):  # Method for string expressions\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]  # Select string columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def other_expr(df):  # Method for other expressions\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]  # Select other columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def count_expr(df):  # Method for count expressions\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]  # Select count columns\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]  # Calculate max\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]  # Calculate last\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def get_expressions(df):  # Method for getting all expressions\n",
    "        exprs = CustomAggregator.numerical_expr(df) + \\\n",
    "                CustomAggregator.date_expr(df) + \\\n",
    "                CustomAggregator.string_expr(df) + \\\n",
    "                CustomAggregator.other_expr(df) + \\\n",
    "                CustomAggregator.count_expr(df)\n",
    "        return exprs\n",
    "\n",
    "def file_reader(path, depth=None):  # Function for reading a single file\n",
    "    df = pl.read_parquet(path)  # Read parquet file\n",
    "    df = df.pipe(CustomPipeline.dtype_table)  # Set table data types\n",
    "    if depth in [1, 2]:  # Aggregate if depth specified\n",
    "        df = df.group_by(\"case_id\").agg(CustomAggregator.get_expressions(df)) \n",
    "    return df\n",
    "\n",
    "def multi_file_reader(regex_path, depth=None):  # Function for reading multiple files\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):  # Iterate over files matching the regex pattern\n",
    "        df = pl.read_parquet(path)  # Read parquet file\n",
    "        df = df.pipe(CustomPipeline.dtype_table)  # Set table data types\n",
    "        if depth in [1, 2]:  # Aggregate if depth specified\n",
    "            df = df.group_by(\"case_id\").agg(CustomAggregator.get_expressions(df))\n",
    "        chunks.append(df)\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\").unique(subset=[\"case_id\"])  # Concatenate DataFrames and drop duplicate rows based on \"case_id\"\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df_base, depth_0, depth_1, depth_2):  # Function for feature engineering\n",
    "    df_base = df_base.with_columns(  # Add month and weekday features based on \"date_decision\"\n",
    "        month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "        weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):  # Join additional depth DataFrames\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(CustomPipeline.date_handling)  # Handle dates\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):  # Function for converting to pandas DataFrame\n",
    "    df_data = df_data.to_pandas()  # Convert Polars DataFrame to pandas DataFrame\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")  # Convert categorical columns to category data type\n",
    "    return df_data, cat_cols\n",
    "\n",
    "def optimize_memory_usage(df):  # Function for optimizing memory usage\n",
    "    start_mem = df.memory_usage().sum() / 1024**2  # Memory usage before optimization\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2  # Memory usage after optimization\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load notebook information and models\n",
    "lgb_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n",
    "cols = lgb_info['cols']\n",
    "cat_cols = lgb_info['cat_cols']\n",
    "file_path_lgb = \"/kaggle/input/homecredlgb2/other/lgb2/1/home-credit-lgb\"\n",
    "file_path_cat = \"/kaggle/input/homecredcat2/other/cat/1/home-credit-lgb\"\n",
    "\n",
    "lgb_models = []\n",
    "cat_models = []\n",
    "\n",
    "for i in range(0, 5): \n",
    "    model_path = f\"{file_path_lgb}/model_{i}.pkl\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        lgb_models.append(model)\n",
    "cat_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n",
    "for i in range(0, 5): \n",
    "    model_path = f\"{file_path_cat}/model_{i}.pkl\"\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "        cat_models.append(model)\n",
    "\n",
    "# Define directory paths\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "# Create a data store dictionary\n",
    "data_store = {\n",
    "    \"df_base\": file_reader(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        file_reader(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        multi_file_reader(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        multi_file_reader(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        multi_file_reader(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        file_reader(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        file_reader(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        multi_file_reader(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        file_reader(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        file_reader(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Perform feature engineering\n",
    "df_test = feature_engineering(**data_store)\n",
    "\n",
    "# Clean up memory\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "# Select columns of interest\n",
    "df_test = df_test.select(['case_id'] + cols)\n",
    "\n",
    "# Convert to pandas DataFrame and optimize memory\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = optimize_memory_usage(df_test)\n",
    "\n",
    "# Set case_id column as index\n",
    "df_test = df_test.set_index('case_id')\n",
    "\n",
    "# Create a VotingModel\n",
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "     \n",
    "    def predict_proba(self, X):      \n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]  # lgb\n",
    "        X[cat_cols] = X[cat_cols].astype(str)\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]  # cat\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(lgb_models + cat_models)\n",
    "\n",
    "# Predict probabilities for the test data\n",
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "\n",
    "# Read the sample submission file\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "# Assign predicted probabilities to the submission dataframe\n",
    "df_subm[\"score\"] = y_pred\n",
    "\n",
    "# Save the submission dataframe to a CSV file\n",
    "df_subm.to_csv(\"submission.csv\")\n",
    "\n",
    "# Display the submission dataframe\n",
    "df_subm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <h1>Conclusion</h1>\n",
    "   <body>\n",
    "      <div class=\"container\">\n",
    "         <div class=\"image-container\">\n",
    "               <div>best place: 101/3572(top 3%)</div>\n",
    "               <a href=\"https://ibb.co/877yVYY\">\n",
    "                  <img src=\"https://i.ibb.co/z443w66/home-cred-1.jpg\" alt=\"home-cred-1\" border=\"0\" width=500 hight=500>\n",
    "               </a>\n",
    "         </div>\n",
    "      </div>\n",
    "         <div>public score: 315/3885(top 8%)</div>\n",
    "               <a href=\"https://ibb.co/MPJ7PXB\">\n",
    "                  <img src=\"https://i.ibb.co/ssXqsBW/last-score-pub.jpg\" alt=\"last-score-pub\" border=\"0\" width=500 hight=500>\n",
    "               </a>\n",
    "         <div>place after approximate: 1559/3885(top 40%)</div>\n",
    "               <a href=\"https://ibb.co/ZgGbC8X\">\n",
    "                  <img src=\"https://i.ibb.co/ykdxJyP/after-appr.jpg\" alt=\"after-appr\" border=\"0\" width=500 hight=500>\n",
    "               </a>\n",
    "   </body>\n",
    "   <h2>Analysis of mistakes</h2>\n",
    "   <p>To be honest, the reason for not achieving the best results, in my opinion, was a lack of experience ( I was focusing on several tournaments at once, spending time on inefficient approaches, and poorly planning my strategy, which resulted in only 40 submissions ), a strange metric, and a bit of bad luck in choosing the answers( I could have chosen an option that would have secured me a 400-900 position (top 10-23%), but since it wouldn't be a medal anyway, I'm not upset :) ) This is an important lesson for me! It was my first serious competition, and my motivation to reach the top in competitive data science has only increased!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image Divider -->\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://t3.ftcdn.net/jpg/07/67/24/58/360_F_767245851_Lw0eYvUcKpFc35XqgHslDTfdjFEdchru.jpg\"  width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>Additional</h1>\n",
    "<h2>Technologies</h2>\n",
    "<p><strong>Python:</strong></p>\n",
    "<ul>\n",
    "  <li>Joblib: Used for saving and loading machine learning models.</li>\n",
    "  <li>Pathlib: Used for path operations with files and directories.</li>\n",
    "  <li>GC (Garbage Collection): Used for memory management and resource deallocation.</li>\n",
    "  <li>Glob: Used for file matching based on patterns.</li>\n",
    "  <li>Numpy: A library for working with multidimensional arrays and mathematical functions.</li>\n",
    "  <li>Pandas: A library for data manipulation and analysis.</li>\n",
    "  <li>Polars: A library for fast data manipulation.</li>\n",
    "  <li>Sklearn (Scikit-learn): A machine learning library with a wide range of tools for classification, regression, clustering, and other tasks.</li>\n",
    "  <li>LightGBM: A gradient boosting framework designed for handling large datasets.</li>\n",
    "  <li>CatBoost: A gradient boosting framework, optimized for working with categorical features.</li>\n",
    "  <li>Pickle: Used for serializing and deserializing Python objects.</li>\n",
    "  <li>Warnings: Used for managing warnings in Python.</li>\n",
    "  <li>Optuna:A hyperparameter optimization framework for automated machine learning.</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Jupyter Notebook:</strong></p>\n",
    "<ul>\n",
    "<li>All the code was written in Jupyter Notebooks, and all notebooks in this format will be available in the project's GitHub repository under the folder named \"notebooks\".</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>HTML:</strong></p>\n",
    "<ul>\n",
    "<li>Was used for project markup.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Where was the competition held?</h2>\n",
    "\n",
    "<p><strong>Competition link (Kaggle):</strong> <a href=\"https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability\">https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability</a></p>\n",
    "<p><strong>My profile link (Kaggle):</strong> <a href=\"https://www.kaggle.com/daniilbokhan\">https://www.kaggle.com/daniilbokhan </a></p>\n",
    "\n",
    "<h2>Sources</h2>\n",
    "\n",
    "<p>Schema jpg - <a href=\"https://www.kaggle.com/code/sergiosaharovskiy/home-credit-crms-2024-eda-and-submission\">https://www.kaggle.com/code/sergiosaharovskiy/home-credit-crms-2024-eda-and-submission</a></p>\n",
    "<p>First steps - <a href=\"https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook\">https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook</a> [moderator of competition]</p>\n",
    "\n",
    "\n",
    "<h3>Thank You for Your Attention!</h3>\n",
    "<p>If you find any errors in the code, please feel free to contact me at daniilbokhan.q@gmail.com. I'm eager to improve!</p>\n",
    "</body>\n",
    "</html>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
